
import subprocess
import json
from pathlib import Path

class SimpleLocalLLM:
    """Simplified local LLM that works with TinyLlama"""
    
    def __init__(self, model_path, executable_path):
        self.model_path = model_path
        self.executable_path = executable_path
    
    def enhance_sentence(self, japanese_sentence, english_sentence):
        """Enhance a sentence using the local LLM"""
        
        # Simple prompt that TinyLlama can handle
        prompt = f"Improve this Japanese sentence: {japanese_sentence} ({english_sentence})"
        
        cmd = [
            self.executable_path,
            "-m", self.model_path,
            "-p", prompt,
            "-n", "50",
            "--temp", "0.7",
            "--no-conversation"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                # Extract the response (remove the prompt)
                response = result.stdout.strip()
                if prompt in response:
                    response = response.replace(prompt, "").strip()
                
                return {
                    "original_japanese": japanese_sentence,
                    "original_english": english_sentence,
                    "enhanced_japanese": response,
                    "enhanced_english": "[Generated by LLM]",
                    "status": "success"
                }
            else:
                return {
                    "original_japanese": japanese_sentence,
                    "original_english": english_sentence,
                    "enhanced_japanese": japanese_sentence,
                    "enhanced_english": english_sentence,
                    "status": "error",
                    "error": result.stderr
                }
                
        except Exception as e:
            return {
                "original_japanese": japanese_sentence,
                "original_english": english_sentence,
                "enhanced_japanese": japanese_sentence,
                "enhanced_english": english_sentence,
                "status": "error",
                "error": str(e)
            }

# Test the simple integration
if __name__ == "__main__":
    project_root = Path(__file__).parent
    models_dir = project_root / "models"
    llama_cli_path = models_dir / "llama.cpp" / "build" / "bin" / "llama-cli"
    model_path = models_dir / "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
    
    llm = SimpleLocalLLM(str(model_path), str(llama_cli_path))
    
    # Test with a generated sentence
    from sentence_generator import SentenceGenerator
    generator = SentenceGenerator("tmp")
    sentence = generator.generate_sentence("identity")
    
    print(f"Original: {sentence.japanese} ({sentence.english})")
    
    result = llm.enhance_sentence(sentence.japanese, sentence.english)
    print(f"Enhanced: {result['enhanced_japanese']}")
    print(f"Status: {result['status']}")
