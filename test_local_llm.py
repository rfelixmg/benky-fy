#!/usr/bin/env python3
"""Test script for local LLM integration."""

import subprocess
import json
from pathlib import Path

def test_llama_cli():
    """Test llama-cli directly"""
    
    # Paths
    project_root = Path(__file__).parent
    models_dir = project_root / "models"
    llama_cli_path = models_dir / "llama.cpp" / "build" / "bin" / "llama-cli"
    model_path = models_dir / "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
    
    print("üß™ Testing llama-cli directly...")
    
    # Simple test prompt
    prompt = "Translate to Japanese: Hello, how are you?"
    
    cmd = [
        str(llama_cli_path),
        "-m", str(model_path),
        "-p", prompt,
        "-n", "30",
        "--temp", "0.7",
        "--no-conversation"
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            print("‚úÖ llama-cli executed successfully!")
            print(f"Output: {result.stdout}")
            return True
        else:
            print(f"‚ùå Error: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Timeout")
        return False
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False

def test_basic_generation():
    """Test basic sentence generation"""
    print("\nüß™ Testing basic sentence generation...")
    
    try:
        from sentence_generator import SentenceGenerator
        
        generator = SentenceGenerator("tmp")
        sentence = generator.generate_sentence("identity", debug=True)
        
        print(f"‚úÖ Generated: {sentence.japanese}")
        print(f"‚úÖ English: {sentence.english}")
        print(f"‚úÖ Coherent: {'Yes' if sentence.coherence_passed else 'No'}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def create_simple_llm_integration():
    """Create a simplified LLM integration that works with TinyLlama"""
    
    print("\nüîß Creating simplified LLM integration...")
    
    # Create a simple wrapper that doesn't require JSON
    simple_llm_code = '''
import subprocess
import json
from pathlib import Path

class SimpleLocalLLM:
    """Simplified local LLM that works with TinyLlama"""
    
    def __init__(self, model_path, executable_path):
        self.model_path = model_path
        self.executable_path = executable_path
    
    def enhance_sentence(self, japanese_sentence, english_sentence):
        """Enhance a sentence using the local LLM"""
        
        # Simple prompt that TinyLlama can handle
        prompt = f"Improve this Japanese sentence: {japanese_sentence} ({english_sentence})"
        
        cmd = [
            self.executable_path,
            "-m", self.model_path,
            "-p", prompt,
            "-n", "50",
            "--temp", "0.7",
            "--no-conversation"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                # Extract the response (remove the prompt)
                response = result.stdout.strip()
                if prompt in response:
                    response = response.replace(prompt, "").strip()
                
                return {
                    "original_japanese": japanese_sentence,
                    "original_english": english_sentence,
                    "enhanced_japanese": response,
                    "enhanced_english": "[Generated by LLM]",
                    "status": "success"
                }
            else:
                return {
                    "original_japanese": japanese_sentence,
                    "original_english": english_sentence,
                    "enhanced_japanese": japanese_sentence,
                    "enhanced_english": english_sentence,
                    "status": "error",
                    "error": result.stderr
                }
                
        except Exception as e:
            return {
                "original_japanese": japanese_sentence,
                "original_english": english_sentence,
                "enhanced_japanese": japanese_sentence,
                "enhanced_english": english_sentence,
                "status": "error",
                "error": str(e)
            }

# Test the simple integration
if __name__ == "__main__":
    project_root = Path(__file__).parent
    models_dir = project_root / "models"
    llama_cli_path = models_dir / "llama.cpp" / "build" / "bin" / "llama-cli"
    model_path = models_dir / "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
    
    llm = SimpleLocalLLM(str(model_path), str(llama_cli_path))
    
    # Test with a generated sentence
    from sentence_generator import SentenceGenerator
    generator = SentenceGenerator("tmp")
    sentence = generator.generate_sentence("identity")
    
    print(f"Original: {sentence.japanese} ({sentence.english})")
    
    result = llm.enhance_sentence(sentence.japanese, sentence.english)
    print(f"Enhanced: {result['enhanced_japanese']}")
    print(f"Status: {result['status']}")
'''
    
    with open("simple_local_llm.py", "w") as f:
        f.write(simple_llm_code)
    
    print("‚úÖ Created simple_local_llm.py")
    return True

if __name__ == "__main__":
    print("üöÄ Local LLM Test Suite")
    print("=" * 50)
    
    # Test basic generation
    if not test_basic_generation():
        print("‚ùå Basic generation failed")
        exit(1)
    
    # Test llama-cli
    if not test_llama_cli():
        print("‚ùå llama-cli test failed")
        exit(1)
    
    # Create simple integration
    if not create_simple_llm_integration():
        print("‚ùå Simple integration creation failed")
        exit(1)
    
    print("\nüéâ All tests passed!")
    print("\nüìã Next steps:")
    print("  1. Run: python simple_local_llm.py")
    print("  2. Test the simplified LLM integration")
    print("  3. Use it for basic sentence enhancement")
